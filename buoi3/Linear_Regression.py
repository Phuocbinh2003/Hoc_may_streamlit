import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from scipy.stats import zscore
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
# Ti√™u ƒë·ªÅ


def tien_xu_ly_du_lieu():
    df = pd.read_csv("buoi2/data.txt")

    # Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt
    columns_to_drop = ["Cabin", "Ticket", "Name"]  # C·ªôt kh√¥ng c·∫ßn thi·∫øt
    df.drop(columns=columns_to_drop, inplace=True)  # Lo·∫°i b·ªè c·ªôt
    # X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
    df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Fare'].fillna(df['Fare'].median(), inplace=True)
    df.dropna(subset=['Embarked'], inplace=True)  # X√≥a d√≤ng n·∫øu 'Embarked' b·ªã thi·∫øu

    # M√£ h√≥a gi·ªõi t√≠nh: Male -> 1, Female -> 0
    df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})

    # M√£ h√≥a 'Embarked' b·∫±ng One-Hot Encoding
    df['Embarked'] = df['Embarked'].map({'Q': 0, 'S': 1, 'C': 2})
    

    # Chu·∫©n h√≥a c√°c gi√° tr·ªã s·ªë
    scaler = StandardScaler()
    df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])

    # Chia d·ªØ li·ªáu th√†nh ƒë·∫ßu v√†o (X) v√† nh√£n (y)
    X = df.drop(columns=['Survived'])
    y = df['Survived']

    # Chia t·∫≠p train (70%), validation (15%), test (15%)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

    # 2Ô∏è‚É£ D√πng StratifiedKFold v·ªõi m·ªói fold ch·ªçn 15% l√†m validation
    kf = StratifiedKFold(n_splits=int(1 / 0.15), shuffle=True, random_state=42)
    return X_train, X_test, y_train, y_test, kf ,df


def gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):
    m, n = X.shape
    X_b = np.c_[np.ones((m, 1)), X]  # Th√™m c·ªôt bias
    w = np.random.randn(n + 1, 1)  # Kh·ªüi t·∫°o tr·ªçng s·ªë ng·∫´u nhi√™n

    for iteration in range(n_iterations):
        gradients = 2/m * X_b.T.dot(X_b.dot(w) - y)
        w -= learning_rate * gradients
    
    return w

def train_multiple_linear_regression(X_train, y_train, learning_rate=0.01, n_iterations=1000):
    """Hu·∫•n luy·ªán h·ªìi quy tuy·∫øn t√≠nh b·ªôi b·∫±ng Gradient Descent."""
    return gradient_descent(X_train, y_train, learning_rate, n_iterations)

def train_polynomial_regression(X_train, y_train, degree=2, learning_rate=0.01, n_iterations=1000):
    """Hu·∫•n luy·ªán h·ªìi quy ƒëa th·ª©c b·∫±ng Gradient Descent."""
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train)
    w = gradient_descent(X_train_poly, y_train, learning_rate, n_iterations)
    return w, poly  # Tr·∫£ v·ªÅ c·∫£ tr·ªçng s·ªë v√† ƒë·ªëi t∆∞·ª£ng poly ƒë·ªÉ transform t·∫≠p test

def chon_mo_hinh(model_type="linear", degree=2, learning_rate=0.01, n_iterations=1000):
    """Ch·ªçn m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi ho·∫∑c h·ªìi quy ƒëa th·ª©c."""
    X_train_full, X_test, y_train_full, y_test, kf, df = tien_xu_ly_du_lieu()
    fold_mse = []
    poly = None

    for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train_full, y_train_full)):
        X_train, X_valid = X_train_full.iloc[train_idx], X_train_full.iloc[valid_idx]
        y_train, y_valid = y_train_full.iloc[train_idx], y_train_full.iloc[valid_idx]

        print(f"\nüöÄ Fold {fold + 1}: Train size = {len(X_train)}, Validation size = {len(X_valid)}")

        if model_type == "linear":
            w = train_multiple_linear_regression(X_train, y_train, learning_rate, n_iterations)
            X_valid_b = np.c_[np.ones((len(X_valid), 1)), X_valid]
            y_valid_pred = X_valid_b.dot(w).flatten()
        elif model_type == "polynomial":
            w, poly = train_polynomial_regression(X_train, y_train, degree, learning_rate, n_iterations)
            X_valid_poly = poly.transform(X_valid)
            X_valid_poly_b = np.c_[np.ones((len(X_valid_poly), 1)), X_valid_poly]
            y_valid_pred = X_valid_poly_b.dot(w).flatten()
        else:
            raise ValueError("‚ö†Ô∏è Ch·ªçn 'linear' ho·∫∑c 'polynomial'!")
        
        mse = mean_squared_error(y_valid, y_valid_pred)
        fold_mse.append(mse)
        print(f"üìå Fold {fold + 1} - MSE: {mse:.4f}")
    
    # üî• Hu·∫•n luy·ªán l·∫°i tr√™n to√†n b·ªô t·∫≠p train_full
    if model_type == "linear":
        final_w = train_multiple_linear_regression(X_train_full, y_train_full, learning_rate, n_iterations)
        X_test_b = np.c_[np.ones((len(X_test), 1)), X_test]
        y_test_pred = X_test_b.dot(final_w).flatten()
    else:
        final_w, poly = train_polynomial_regression(X_train_full, y_train_full, degree, learning_rate, n_iterations)
        X_test_poly = poly.transform(X_test)
        X_test_poly_b = np.c_[np.ones((len(X_test_poly), 1)), X_test_poly]
        y_test_pred = X_test_poly_b.dot(final_w).flatten()
    
    test_mse = mean_squared_error(y_test, y_test_pred)
    avg_mse = np.mean(fold_mse)
    
    st.success(f"MSE trung b√¨nh qua c√°c folds: {avg_mse:.4f}")
    st.success(f"MSE tr√™n t·∫≠p test: {test_mse:.4f}")
    
    return final_w, avg_mse, poly

def bt_buoi3():
    uploaded_file = "buoi2/data.txt"
    try:
        df = pd.read_csv(uploaded_file, delimiter=",")
    except FileNotFoundError:
        st.error("‚ùå Kh√¥ng t√¨m th·∫•y t·ªáp d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")
        st.stop()
    st.title("üîç Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu")
    
    st.subheader("üìå 10 d√≤ng ƒë·∫ßu c·ªßa d·ªØ li·ªáu g·ªëc")
    st.write(df.head(10))
    
    st.subheader("üö® Ki·ªÉm tra l·ªói d·ªØ li·ªáu")

                # Ki·ªÉm tra gi√° tr·ªã thi·∫øu
    missing_values = df.isnull().sum()

                # Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p
    duplicate_count = df.duplicated().sum()

                
                
                # Ki·ªÉm tra gi√° tr·ªã qu√° l·ªõn (outlier) b·∫±ng Z-score
    outlier_count = {
        col: (abs(zscore(df[col], nan_policy='omit')) > 3).sum()
        for col in df.select_dtypes(include=['number']).columns
    }

                # T·∫°o b√°o c√°o l·ªói
    error_report = pd.DataFrame({
        'C·ªôt': df.columns,
        'Gi√° tr·ªã thi·∫øu': missing_values,
        'Outlier': [outlier_count.get(col, 0) for col in df.columns]
    })

                # Hi·ªÉn th·ªã b√°o c√°o l·ªó
    st.table(error_report)

                # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng d·ªØ li·ªáu tr√πng l·∫∑p
    st.write(f"üîÅ **S·ªë l∆∞·ª£ng d√≤ng b·ªã tr√πng l·∫∑p:** {duplicate_count}")         
    
    st.title("üîç Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu")

    # Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt
    st.subheader("1Ô∏è‚É£ Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng quan tr·ªçng")
    st.write("""
    M·ªôt s·ªë c·ªôt trong d·ªØ li·ªáu c√≥ th·ªÉ kh√¥ng ƒë√≥ng g√≥p nhi·ªÅu v√†o k·∫øt qu·∫£ d·ª± ƒëo√°n ho·∫∑c ch·ª©a qu√° nhi·ªÅu gi√° tr·ªã thi·∫øu. Vi·ªác lo·∫°i b·ªè c√°c c·ªôt n√†y gi√∫p gi·∫£m ƒë·ªô ph·ª©c t·∫°p c·ªßa m√¥ h√¨nh v√† c·∫£i thi·ªán hi·ªáu su·∫•t.
    """)

    # X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
    st.subheader("2Ô∏è‚É£ X·ª≠ l√Ω gi√° tr·ªã thi·∫øu")
    st.write("""
    D·ªØ li·ªáu th·ª±c t·∫ø th∆∞·ªùng ch·ª©a c√°c gi√° tr·ªã b·ªã thi·∫øu. Ta c·∫ßn l·ª±a ch·ªçn ph∆∞∆°ng ph√°p th√≠ch h·ª£p nh∆∞ ƒëi·ªÅn gi√° tr·ªã trung b√¨nh, lo·∫°i b·ªè h√†ng ho·∫∑c s·ª≠ d·ª•ng m√¥ h√¨nh d·ª± ƒëo√°n ƒë·ªÉ x·ª≠ l√Ω ch√∫ng nh·∫±m tr√°nh ·∫£nh h∆∞·ªüng ƒë·∫øn m√¥ h√¨nh.
    """)

    # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
    st.subheader("3Ô∏è‚É£ Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu")
    st.write("""
    M·ªôt s·ªë c·ªôt trong d·ªØ li·ªáu c√≥ th·ªÉ ch·ª©a gi√° tr·ªã d·∫°ng ch·ªØ (danh m·ª•c). ƒê·ªÉ m√¥ h√¨nh c√≥ th·ªÉ x·ª≠ l√Ω, ta c·∫ßn chuy·ªÉn ƒë·ªïi ch√∫ng th√†nh d·∫°ng s·ªë b·∫±ng c√°c ph∆∞∆°ng ph√°p nh∆∞ one-hot encoding ho·∫∑c label encoding.
    """)

    # Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë
    st.subheader("4Ô∏è‚É£ Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë")
    st.write("""
    C√°c gi√° tr·ªã s·ªë trong t·∫≠p d·ªØ li·ªáu c√≥ th·ªÉ c√≥ ph·∫°m vi r·∫•t kh√°c nhau, ƒëi·ªÅu n√†y c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë·ªô h·ªôi t·ª• c·ªßa m√¥ h√¨nh. Ta c·∫ßn chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng c√≥ c√πng tr·ªçng s·ªë khi hu·∫•n luy·ªán m√¥ h√¨nh.
    """)

    # Chia d·ªØ li·ªáu th√†nh t·∫≠p Train, Validation, v√† Test
    st.subheader("5Ô∏è‚É£ Chia d·ªØ li·ªáu th√†nh t·∫≠p Train, Validation, v√† Test")
    st.write("""
    ƒê·ªÉ ƒë·∫£m b·∫£o m√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët tr√™n d·ªØ li·ªáu th·ª±c t·∫ø, ta chia t·∫≠p d·ªØ li·ªáu th√†nh ba ph·∫ßn:
    - **Train**: D√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh.
    - **Validation**: D√πng ƒë·ªÉ ƒëi·ªÅu ch·ªânh tham s·ªë m√¥ h√¨nh nh·∫±m t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t.
    - **Test**: D√πng ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t cu·ªëi c√πng c·ªßa m√¥ h√¨nh tr√™n d·ªØ li·ªáu ch∆∞a t·ª´ng th·∫•y.
    """)
    
    
    
    st.title("L·ª±a ch·ªçn thu·∫≠t to√°n h·ªçc m√°y: Multiple vs. Polynomial Regression")

    # Gi·ªõi thi·ªáu
    st.write("## 1. Multiple Linear Regression")
    st.write("""
    H·ªìi quy tuy·∫øn t√≠nh b·ªôi l√† m·ªôt thu·∫≠t to√°n h·ªçc m√°y c√≥ gi√°m s√°t, m√¥ t·∫£ m·ªëi quan h·ªá gi·ªØa m·ªôt bi·∫øn ph·ª• thu·ªôc (output) v√† nhi·ªÅu bi·∫øn ƒë·ªôc l·∫≠p (input) th√¥ng qua m·ªôt h√†m tuy·∫øn t√≠nh.
    V√≠ d·ª• d·ª± ƒëo√°n gi√° nh√† d·ª±a tr√™n di·ªán t√≠ch, s·ªë ph√≤ng, v·ªã tr√≠, ... 
    
    C√¥ng th·ª©c t·ªïng qu√°t c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi:
    """)
    st.image("buoi3/img1.png", caption="Multiple Linear Regression ƒë∆°n", use_container_width =True)
    st.latex(r"""
    y = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n
    """)

    
   
   

    # Gi·ªõi thi·ªáu Polynomial Regression
    st.write("## 2. Polynomial Regression")

    st.write("Polynomial Regression m·ªü r·ªông m√¥ h√¨nh tuy·∫øn t√≠nh b·∫±ng c√°ch th√™m c√°c b·∫≠c cao h∆°n c·ªßa bi·∫øn ƒë·∫ßu v√†o.")
    
    st.image("buoi3/img3.png", caption="Polynomial Regression ", use_container_width =True)
    st.write("""
     C√¥ng th·ª©c t·ªïng qu√°t c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi:
    """)
    st.latex(r"""
    y = w_0 + w_1x + w_2x^2 + w_3x^3 + \dots + w_nx^n
    """)

    
    st.write("""
    ### H√†m m·∫•t m√°t (Loss Function) c·ªßa Linear Regression
    H√†m m·∫•t m√°t ph·ªï bi·∫øn nh·∫•t l√† **Mean Squared Error (MSE)**:
    """)
    st.latex(r"""
    MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
    """)

    st.markdown(r"""
    Trong ƒë√≥:
    - $n$: S·ªë l∆∞·ª£ng ƒëi·ªÉm d·ªØ li·ªáu.
    - $y_i$: Gi√° tr·ªã th·ª±c t·∫ø c·ªßa bi·∫øn ph·ª• thu·ªôc.
    - $\hat{y}_i$: Gi√° tr·ªã d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh.
    """)

    st.markdown(r"""
    M·ª•c ti√™u c·ªßa h·ªìi quy tuy·∫øn t√≠nh b·ªôi l√† t√¨m c√°c h·ªá s·ªë tr·ªçng s·ªë $w_0, w_1, w_2, ..., w_n$ sao cho gi√° tr·ªã MSE nh·ªè nh·∫•t.

    ### Thu·∫≠t to√°n Gradient Descent
    1. Kh·ªüi t·∫°o c√°c tr·ªçng s·ªë $w_0, w_1, w_2, ..., w_n$ v·ªõi gi√° tr·ªã b·∫•t k·ª≥.
    2. T√≠nh gradient c·ªßa MSE ƒë·ªëi v·ªõi t·ª´ng tr·ªçng s·ªë.
    3. C·∫≠p nh·∫≠t tr·ªçng s·ªë theo quy t·∫Øc c·ªßa thu·∫≠t to√°n Gradient Descent.

    ### ƒê√°nh gi√° m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi
    - **H·ªá s·ªë t∆∞∆°ng quan (R)**: ƒê√°nh gi√° m·ª©c ƒë·ªô t∆∞∆°ng quan gi·ªØa gi√° tr·ªã th·ª±c t·∫ø v√† gi√° tr·ªã d·ª± ƒëo√°n.
    - **H·ªá s·ªë x√°c ƒë·ªãnh (R¬≤)**: ƒêo l∆∞·ªùng ph·∫ßn trƒÉm bi·∫øn ƒë·ªông c·ªßa bi·∫øn ph·ª• thu·ªôc c√≥ th·ªÉ gi·∫£i th√≠ch b·ªüi c√°c bi·∫øn ƒë·ªôc l·∫≠p:
    """)
    st.latex(r"""
    R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
    """)

    st.write("""
    - **Adjusted R¬≤**: ƒêi·ªÅu ch·ªânh cho s·ªë l∆∞·ª£ng bi·∫øn ƒë·ªôc l·∫≠p, gi√∫p tr√°nh overfitting:
    """)
    st.latex(r"""
    R^2_{adj} = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - k - 1} \right)
    """)

    st.markdown(r"""
    Trong ƒë√≥:
    - $n$: S·ªë l∆∞·ª£ng quan s√°t.
    - $k$: S·ªë l∆∞·ª£ng bi·∫øn ƒë·ªôc l·∫≠p.
    - $\bar{y}$: Gi√° tr·ªã trung b√¨nh c·ªßa bi·∫øn ph·ª• thu·ªôc.
    """)

    st.write("""
    
    - **Sai s·ªë chu·∫©n (SE)**: ƒê√°nh gi√° m·ª©c ƒë·ªô ph√¢n t√°n c·ªßa sai s·ªë d·ª± ƒëo√°n quanh gi√° tr·ªã th·ª±c t·∫ø:
    """)
    st.latex(r"""
    SE = \sqrt{\frac{\sum (y_i - \hat{y}_i)^2}{n - k - 1}}
    """)

    st.write("""
    C√°c ch·ªâ s·ªë n√†y gi√∫p ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c v√† kh·∫£ nƒÉng kh√°i qu√°t h√≥a c·ªßa m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh b·ªôi.
    """)
    # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
    st.write("## 3. Minh h·ªça tr·ª±c quan")

    # T·∫°o d·ªØ li·ªáu m·∫´u
    np.random.seed(0)
    x = np.sort(5 * np.random.rand(20, 1), axis=0)
    y = 2 * x**2 - 3 * x + np.random.randn(20, 1) * 2

    # H·ªìi quy tuy·∫øn t√≠nh
    lin_reg = LinearRegression()
    lin_reg.fit(x, y)
    y_pred_linear = lin_reg.predict(x)

    # H·ªìi quy b·∫≠c hai
    poly_features = PolynomialFeatures(degree=2)
    x_poly = poly_features.fit_transform(x)
    poly_reg = LinearRegression()
    poly_reg.fit(x_poly, y)
    y_pred_poly = poly_reg.predict(x_poly)

    # V·∫Ω bi·ªÉu ƒë·ªì
    fig, ax = plt.subplots(figsize=(6, 4))
    ax.scatter(x, y, color='blue', label='D·ªØ li·ªáu th·ª±c t·∫ø')
    ax.plot(x, y_pred_linear, color='red', label='Multiple Linear Regression')
    ax.plot(x, y_pred_poly, color='green', label='Polynomial Regression (b·∫≠c 2)')
    ax.set_xlabel("X")
    ax.set_ylabel("Y")
    ax.legend()
    st.pyplot(fig)
    
    X_train_full, X_test, y_train_full, y_test, kf, df = tien_xu_ly_du_lieu()
    st.write(df.head(10))

    # Ch·ªçn lo·∫°i m√¥ h√¨nh
    model_type = st.radio("Ch·ªçn lo·∫°i m√¥ h√¨nh:", ["Multiple Linear Regression", "Polynomial Regression"])

    # N·∫øu ch·ªçn Polynomial Regression, cho ph√©p ch·ªçn b·∫≠c ƒëa th·ª©c
    degree = 2
    if model_type == "Polynomial Regression":
        degree = st.slider("Ch·ªçn b·∫≠c c·ªßa h·ªìi quy ƒëa th·ª©c:", min_value=2, max_value=5, value=2)

    # Khi nh·∫•n n√∫t s·∫Ω hu·∫•n luy·ªán m√¥ h√¨nh
    if st.button("Hu·∫•n luy·ªán m√¥ h√¨nh"):
        model, avg_mse, poly = chon_mo_hinh(
            model_type="linear" if model_type == "Multiple Linear Regression" else "polynomial",
            degree=degree
        )

        # Hi·ªÉn th·ªã k·∫øt qu·∫£ hu·∫•n luy·ªán
        st.success(f"MSE trung b√¨nh qua c√°c folds: {avg_mse:.4f}")

        # N·∫øu l√† Polynomial Regression, hi·ªÉn th·ªã th√™m b·∫≠c c·ªßa m√¥ h√¨nh
        if model_type == "Polynomial Regression":
            st.write(f"‚úÖ M√¥ h√¨nh h·ªìi quy b·∫≠c {degree} ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán th√†nh c√¥ng!")
    
    
if __name__ == "__main__":
    bt_buoi3()