import os
import subprocess
import time
import mlflow
import streamlit as st
from mlflow.tracking import MlflowClient
from langchain_openai import OpenAI
from langchain_core.prompts import PromptTemplate

def appptest():
    # C·∫•u h√¨nh MLflow v·ªõi th∆∞ m·ª•c l∆∞u tr·ªØ ph√π h·ª£p
    mlflow.set_tracking_uri("file:D:/Hoc_may/mlruns")
    
    # Experiment ID
    experiment_id = "251068899510733485"

    # Ki·ªÉm tra xem Experiment c√≥ t·ªìn t·∫°i kh√¥ng
    client = MlflowClient()
    experiment = client.get_experiment(experiment_id)

    if experiment is None:
        st.error(f"Kh√¥ng t√¨m th·∫•y experiment ID: {experiment_id}. Ki·ªÉm tra l·∫°i mlruns!")
        return  # D·ª´ng n·∫øu kh√¥ng t√¨m th·∫•y

    mlflow.set_experiment(experiment_id=experiment_id)

    # H√†m kh·ªüi ƒë·ªông MLflow UI trong n·ªÅn
    def start_mlflow_ui():
        try:
            subprocess.Popen(["mlflow", "ui", "--backend-store-uri", "file:D:/Hoc_may/mlruns", "--port", "5000"], 
                             stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            time.sleep(3)  # Ch·ªù v√†i gi√¢y ƒë·ªÉ UI kh·ªüi ƒë·ªông
        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông MLflow UI: {e}")

    # Ch·∫°y MLflow UI khi m·ªü ·ª©ng d·ª•ng Streamlit
    start_mlflow_ui()

    # Giao di·ªán Streamlit
    st.title("MLflow LangChain Tracking v·ªõi Streamlit")

    # Hi·ªÉn th·ªã link truy c·∫≠p MLflow UI
    st.markdown("### üîó [Truy c·∫≠p MLflow UI](http://localhost:5000)")

    # ƒê·∫∑t API Key c·ªßa OpenAI t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    os.environ["OPENAI_API_KEY"] = "your_openai_api_key_here"

    # Kh·ªüi t·∫°o LangChain Model
    llm = OpenAI()
    prompt = PromptTemplate.from_template("Answer the following question: {question}")
    chain = prompt | llm

    # C√¢u h·ªèi demo
    question = st.text_input("Nh·∫≠p c√¢u h·ªèi:", "What is MLflow?")

    if st.button("G·ª≠i c√¢u h·ªèi"):
        with mlflow.start_run():
            response = chain.invoke(question)

            # Ghi log v√†o MLflow
            mlflow.log_param("prompt", "Answer the following question: {question}")
            mlflow.log_param("question", question)
            mlflow.log_param("model", "OpenAI GPT")
            mlflow.log_metric("response_length", len(response))
            mlflow.log_text(response, "response.txt")

            st.write("### Ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh:")
            st.write(response)

# G·ªçi h√†m khi ch·∫°y ·ª©ng d·ª•ng Streamlit
if __name__ == "__main__":
    appptest()
