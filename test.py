import os
import subprocess
import time
import mlflow
import streamlit as st
from langchain_openai import OpenAI
from langchain_core.prompts import PromptTemplate
def appptest():
    
    # ƒê·∫∑t s·∫µn API Key c·ªßa OpenAI (ho·∫∑c b·∫°n c√≥ th·ªÉ l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng)
    os.environ["OPENAI_API_KEY"] = "your_openai_api_key_here"

    # Kh·ªüi t·∫°o MLflow v·ªõi experiment ID
    experiment_id = "251068899510733485"
    mlflow.set_experiment(experiment_id=experiment_id)

    # H√†m kh·ªüi ƒë·ªông MLflow UI trong n·ªÅn
    def start_mlflow_ui():
        try:
            subprocess.Popen(["mlflow", "ui", "--port", "5000"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            time.sleep(3)  # Ch·ªù v√†i gi√¢y ƒë·ªÉ UI kh·ªüi ƒë·ªông
        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông MLflow UI: {e}")

    # Ch·∫°y MLflow UI khi m·ªü ·ª©ng d·ª•ng Streamlit
    start_mlflow_ui()

    st.title("MLflow LangChain Tracking v·ªõi Streamlit")

    # Hi·ªÉn th·ªã link truy c·∫≠p MLflow UI
    st.markdown("### üîó [Truy c·∫≠p MLflow UI](http://localhost:5000)")

    # G·ªçi LangChain ƒë·ªÉ ghi log v√†o MLflow
    with mlflow.start_run():
        llm = OpenAI()
        prompt = PromptTemplate.from_template("Answer the following question: {question}")
        chain = prompt | llm

        # C√¢u h·ªèi demo
        question = st.text_input("Nh·∫≠p c√¢u h·ªèi:", "What is MLflow?")
        
        if st.button("G·ª≠i c√¢u h·ªèi"):
            response = chain.invoke(question)

            # Ghi log v√†o MLflow
            mlflow.log_param("prompt", "Answer the following question: {question}")
            mlflow.log_param("question", question)
            mlflow.log_param("model", "OpenAI GPT")
            mlflow.log_metric("response_length", len(response))
            mlflow.log_text(response, "response.txt")

            st.write("### Ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh:")
            st.write(response)
