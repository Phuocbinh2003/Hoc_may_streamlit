import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.datasets import fetch_openml

def explain_pca():
    st.markdown("## üß† Hi·ªÉu PCA m·ªôt c√°ch ƒë∆°n gi·∫£n")

    st.markdown("""
    **PCA (Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh)** l√† m·ªôt ph∆∞∆°ng ph√°p gi√∫p gi·∫£m s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu m√† v·∫´n gi·ªØ ƒë∆∞·ª£c th√¥ng tin quan tr·ªçng nh·∫•t.  
    H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu nhi·ªÅu chi·ªÅu (nhi·ªÅu c·ªôt), nh∆∞ng b·∫°n mu·ªën bi·ªÉu di·ªÖn n√≥ trong kh√¥ng gian 2D ho·∫∑c 3D ƒë·ªÉ d·ªÖ hi·ªÉu h∆°n. PCA gi√∫p b·∫°n l√†m ƒëi·ªÅu ƒë√≥!  

    ### üîπ **V√≠ d·ª• tr·ª±c quan**:
    H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu g·ªìm nhi·ªÅu ƒëi·ªÉm ph√¢n b·ªë theo m·ªôt ƒë∆∞·ªùng ch√©o trong kh√¥ng gian 2D:
    """)

   
    np.random.seed(42)
    x = np.random.rand(100) * 10  
    y = x * 0.8 + np.random.randn(100) * 2  
    X = np.column_stack((x, y))

    fig, ax = plt.subplots()
    ax.scatter(X[:, 0], X[:, 1], color="blue", alpha=0.5, label="D·ªØ li·ªáu ban ƒë·∫ßu")
    ax.set_xlabel("X1")
    ax.set_ylabel("X2")
    ax.legend()
    st.pyplot(fig)

    st.markdown(r"""
    ## üìå PCA - Gi·∫£i th√≠ch Tr·ª±c Quan  
    D·ªØ li·ªáu n√†y c√≥ s·ª± ph√¢n t√°n r√µ r√†ng theo m·ªôt h∆∞·ªõng ch√≠nh. PCA s·∫Ω t√¨m ra h∆∞·ªõng ƒë√≥ ƒë·ªÉ bi·ªÉu di·ªÖn d·ªØ li·ªáu m·ªôt c√°ch t·ªëi ∆∞u.

    ---

    ### üîπ **C√°c b∆∞·ªõc th·ª±c hi·ªán PCA d·ªÖ hi·ªÉu**

    1Ô∏è‚É£ **T√¨m ƒëi·ªÉm trung t√¢m (mean vector)**  
    - Tr∆∞·ªõc ti√™n, t√≠nh gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng ƒë·∫∑c tr∆∞ng (feature) trong t·∫≠p d·ªØ li·ªáu.  
    - Vector trung b√¨nh n√†y gi√∫p x√°c ƒë·ªãnh "trung t√¢m" c·ªßa d·ªØ li·ªáu.  
    $$ 
    \mu = \frac{1}{n} \sum_{i=1}^{n} x_i 
    $$  
    - Trong ƒë√≥:
        - \( n \) l√† s·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu.
        - \( x_i \) l√† t·ª´ng ƒëi·ªÉm d·ªØ li·ªáu.

    2Ô∏è‚É£ **D·ªãch chuy·ªÉn d·ªØ li·ªáu v·ªÅ g·ªëc t·ªça ƒë·ªô**  
    - ƒê·ªÉ ƒë·∫£m b·∫£o ph√¢n t√≠ch ch√≠nh x√°c h∆°n, ta d·ªãch chuy·ªÉn d·ªØ li·ªáu sao cho trung t√¢m c·ªßa n√≥ n·∫±m t·∫°i g·ªëc t·ªça ƒë·ªô b·∫±ng c√°ch tr·ª´ ƒëi vector trung b√¨nh:  
    $$ 
    X_{\text{norm}} = X - \mu
    $$  
    - Khi ƒë√≥, d·ªØ li·ªáu s·∫Ω c√≥ gi√° tr·ªã trung b√¨nh b·∫±ng 0.

    3Ô∏è‚É£ **T√≠nh ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai**  
    - Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai gi√∫p ƒëo l∆∞·ªùng m·ª©c ƒë·ªô bi·∫øn thi√™n gi·ªØa c√°c ƒë·∫∑c tr∆∞ng:  
    $$ 
    C = \frac{1}{n} X_{\text{norm}}^T X_{\text{norm}}
    $$  
    - √ù nghƒ©a:
        - N·∫øu ph·∫ßn t·ª≠ \( C_{ij} \) c√≥ gi√° tr·ªã l·ªõn ‚Üí Hai ƒë·∫∑c tr∆∞ng \( i \) v√† \( j \) c√≥ m·ªëi t∆∞∆°ng quan m·∫°nh.
        - N·∫øu \( C_{ij} \) g·∫ßn 0 ‚Üí Hai ƒë·∫∑c tr∆∞ng kh√¥ng li√™n quan nhi·ªÅu.

    4Ô∏è‚É£ **T√¨m c√°c h∆∞·ªõng quan tr·ªçng nh·∫•t**  
    - T√≠nh tr·ªã ri√™ng (eigenvalues) v√† vector ri√™ng (eigenvectors) t·ª´ ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai:  
    $$ 
    C v = \lambda v
    $$  
    - Trong ƒë√≥:
        - \( v \) l√† vector ri√™ng (eigenvector) - ƒë·∫°i di·ªán cho c√°c h∆∞·ªõng ch√≠nh c·ªßa d·ªØ li·ªáu.
        - \( \lambda \) l√† tr·ªã ri√™ng (eigenvalue) - th·ªÉ hi·ªán ƒë·ªô quan tr·ªçng c·ªßa t·ª´ng h∆∞·ªõng.
    - Vector ri√™ng c√≥ tr·ªã ri√™ng l·ªõn h∆°n s·∫Ω mang nhi·ªÅu th√¥ng tin quan tr·ªçng h∆°n.

    5Ô∏è‚É£ **Ch·ªçn s·ªë chi·ªÅu m·ªõi v√† t·∫°o kh√¥ng gian con**  
    - Ch·ªçn \( K \) vector ri√™ng t∆∞∆°ng ·ª©ng v·ªõi \( K \) tr·ªã ri√™ng l·ªõn nh·∫•t ƒë·ªÉ t·∫°o ma tr·∫≠n \( U_K \):  
    $$ 
    U_K = [v_1, v_2, ..., v_K]
    $$  
    - C√°c vector n√†y t·∫°o th√†nh h·ªá tr·ª±c giao v√† gi√∫p ta bi·ªÉu di·ªÖn d·ªØ li·ªáu t·ªëi ∆∞u trong kh√¥ng gian m·ªõi.

    6Ô∏è‚É£ **Chi·∫øu d·ªØ li·ªáu v√†o kh√¥ng gian m·ªõi**  
    - Bi·ªÉu di·ªÖn d·ªØ li·ªáu trong h·ªá tr·ª•c m·ªõi b·∫±ng c√°ch nh√¢n d·ªØ li·ªáu chu·∫©n h√≥a v·ªõi ma tr·∫≠n \( U_K \):  
    $$ 
    X_{\text{new}} = X_{\text{norm}} U_K
    $$  
    - D·ªØ li·ªáu m·ªõi \( X_{\text{new}} \) c√≥ s·ªë chi·ªÅu √≠t h∆°n nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c nhi·ªÅu th√¥ng tin quan tr·ªçng.

    7Ô∏è‚É£ **D·ªØ li·ªáu m·ªõi ch√≠nh l√† t·ªça ƒë·ªô c·ªßa c√°c ƒëi·ªÉm trong kh√¥ng gian m·ªõi.**  
    - M·ªói ƒëi·ªÉm d·ªØ li·ªáu gi·ªù ƒë√¢y ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng c√°c th√†nh ph·∫ßn ch√≠nh thay v√¨ c√°c ƒë·∫∑c tr∆∞ng ban ƒë·∫ßu.

    ---

    ### üîπ **Tr·ª±c quan h√≥a qu√° tr√¨nh PCA**  
    D∆∞·ªõi ƒë√¢y l√† minh h·ªça c√°ch PCA t√¨m ra tr·ª•c quan tr·ªçng nh·∫•t c·ªßa d·ªØ li·ªáu:
    """)



    # PCA th·ªß c√¥ng
    X_centered = X - np.mean(X, axis=0)
    cov_matrix = np.cov(X_centered.T)
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

    fig, ax = plt.subplots()
    ax.scatter(X[:, 0], X[:, 1], color="blue", alpha=0.5, label="D·ªØ li·ªáu ban ƒë·∫ßu")
    origin = np.mean(X, axis=0)

    for i in range(2):
        ax.arrow(origin[0], origin[1], 
                 eigenvectors[0, i] * 3, eigenvectors[1, i] * 3, 
                 head_width=0.3, head_length=0.3, color="red", label=f"Tr·ª•c {i+1}")

    ax.set_xlabel("X1")
    ax.set_ylabel("X2")
    ax.legend()
    st.pyplot(fig)

    st.markdown("""
    **üîπ K·∫øt qu·∫£:**  
    - Tr·ª•c ƒë·ªè l√† h∆∞·ªõng m√† PCA t√¨m ra.  
    - N·∫øu ch·ªçn 1 tr·ª•c ch√≠nh, ta c√≥ th·ªÉ chi·∫øu d·ªØ li·ªáu l√™n n√≥ ƒë·ªÉ gi·∫£m chi·ªÅu.  
      
    Nh·ªù ƒë√≥, ch√∫ng ta c√≥ th·ªÉ bi·ªÉu di·ªÖn d·ªØ li·ªáu m·ªôt c√°ch g·ªçn g√†ng h∆°n m√† kh√¥ng m·∫•t qu√° nhi·ªÅu th√¥ng tin!  
    """)


def explain_tsne():
    st.markdown(r"""
    ## üåå t-Distributed Stochastic Neighbor Embedding (t-SNE)
    t-SNE l√† m·ªôt ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu m·∫°nh m·∫Ω, gi√∫p hi·ªÉn th·ªã d·ªØ li·ªáu ƒëa chi·ªÅu tr√™n m·∫∑t ph·∫≥ng 2D ho·∫∑c kh√¥ng gian 3D b·∫±ng c√°ch b·∫£o to√†n m·ªëi quan h·ªá gi·ªØa c√°c ƒëi·ªÉm g·∫ßn nhau.

    ### üîπ **Nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa t-SNE:**
    1. **T√≠nh x√°c su·∫•t ƒëi·ªÉm g·∫ßn nhau trong kh√¥ng gian g·ªëc:**  
       V·ªõi m·ªói ƒëi·ªÉm $x_i$, ta ƒë·ªãnh nghƒ©a x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán gi·ªØa $x_i$ v√† $x_j$ nh∆∞ sau:
       $$
       p_{j|i} = \frac{\exp(-\| x_i - x_j \|^2 / 2\sigma^2)}{\sum_{k \neq i} \exp(-\| x_i - x_k \|^2 / 2\sigma^2)}
       $$
       Trong ƒë√≥, $\sigma$ l√† tham s·ªë ·∫£nh h∆∞·ªüng ƒë·∫øn m·ª©c ƒë·ªô ph√¢n b·ªë c·ªßa ƒëi·ªÉm xung quanh.

    2. **T√≠nh x√°c su·∫•t trong kh√¥ng gian gi·∫£m chi·ªÅu (2D/3D):**  
       Trong kh√¥ng gian gi·∫£m chi·ªÅu, ta s·ª≠ d·ª•ng ph√¢n ph·ªëi t-Student v·ªõi m·ªôt m·ª©c ƒë·ªô t·ª± do:
       $$
       q_{j|i} = \frac{(1 + \| y_i - y_j \|^2)^{-1}}{\sum_{k \neq i} (1 + \| y_i - y_k \|^2)^{-1}}
       $$

    3. **T·ªëi ∆∞u h√≥a kho·∫£ng c√°ch gi·ªØa $p_{j|i}$ v√† $q_{j|i}$:**  
       t-SNE c·ªë g·∫Øng l√†m cho $p_{j|i}$ trong kh√¥ng gian g·ªëc g·∫ßn b·∫±ng $q_{j|i}$ trong kh√¥ng gian m·ªõi, b·∫±ng c√°ch gi·∫£m **h√†m m·∫•t m√°t Kullback-Leibler (KL divergence):**
       $$
       KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
       $$

    ### ‚úÖ **∆Øu ƒëi·ªÉm c·ªßa t-SNE**
    - Hi·ªÉn th·ªã c·ª•m d·ªØ li·ªáu t·ªët h∆°n PCA.
    - B·∫£o to√†n m·ªëi quan h·ªá phi tuy·∫øn t√≠nh.

    ### ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm c·ªßa t-SNE**
    - Ch·∫°y ch·∫≠m h∆°n PCA.
    - Nh·∫°y c·∫£m v·ªõi c√°c tham s·ªë nh∆∞ perplexity.
    """)
def thi_nghiem():
    
    st.title("üìâ Gi·∫£m chi·ªÅu d·ªØ li·ªáu MNIST v·ªõi PCA & t-SNE")

    # Load d·ªØ li·ªáu
    Xmt = np.load("buoi4/X.npy")
    ymt = np.load("buoi4/y.npy")
    X = Xmt.reshape(Xmt.shape[0], -1) 
    y = ymt.reshape(-1) 






    # T√πy ch·ªçn thu·∫≠t to√°n
    method = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu", ["PCA", "t-SNE"])
    n_components = st.slider("S·ªë chi·ªÅu gi·∫£m xu·ªëng", 2, 3, 2)

    # Gi·ªõi h·∫°n s·ªë m·∫´u ƒë·ªÉ tƒÉng t·ªëc (c√≥ th·ªÉ ch·ªânh s·ª≠a)
    num_samples = 5000
    X_subset, y_subset = X[:num_samples], y[:num_samples]

    # N√∫t ch·∫°y thu·∫≠t to√°n
    if st.button("üöÄ Ch·∫°y gi·∫£m chi·ªÅu"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            if method == "PCA":
                reducer = PCA(n_components=n_components)
            else:
                reducer = TSNE(n_components=n_components, perplexity=min(30, num_samples - 1), random_state=42)

            X_reduced = reducer.fit_transform(X_subset)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            if n_components == 2:
                fig = px.scatter(x=X_reduced[:, 0], y=X_reduced[:, 1], color=y_subset.astype(str),
                                title=f"{method} gi·∫£m chi·ªÅu xu·ªëng {n_components}D",
                                labels={'x': "Th√†nh ph·∫ßn 1", 'y': "Th√†nh ph·∫ßn 2"})
            else:  # Bi·ªÉu ƒë·ªì 3D
                fig = px.scatter_3d(x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2],
                                    color=y_subset.astype(str),
                                    title=f"{method} gi·∫£m chi·ªÅu xu·ªëng {n_components}D",
                                    labels={'x': "Th√†nh ph·∫ßn 1", 'y': "Th√†nh ph·∫ßn 2", 'z': "Th√†nh ph·∫ßn 3"})

            st.plotly_chart(fig)

    st.success("Ho√†n th√†nh!")
    
    
def pca_tsne():
        
    tab1, tab2, tab3 = st.tabs(["üìò L√Ω thuy·∫øt PCA", "üìò L√Ω thuy·∫øt t-NSE", "üìò Data"] )

    with tab1:
        explain_pca()

    with tab2:
        explain_tsne()
    
    with tab3:
        thi_nghiem()



if __name__ == "__main__":
    pca_tsne()