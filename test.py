import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.datasets import fetch_openml

def explain_pca():
    st.markdown("## üß† Hi·ªÉu PCA m·ªôt c√°ch ƒë∆°n gi·∫£n")

    st.markdown("""
    **PCA (Ph√¢n t√≠ch th√†nh ph·∫ßn ch√≠nh)** l√† m·ªôt ph∆∞∆°ng ph√°p gi√∫p gi·∫£m s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu m√† v·∫´n gi·ªØ ƒë∆∞·ª£c th√¥ng tin quan tr·ªçng nh·∫•t.  
    H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu nhi·ªÅu chi·ªÅu (nhi·ªÅu c·ªôt), nh∆∞ng b·∫°n mu·ªën bi·ªÉu di·ªÖn n√≥ trong kh√¥ng gian 2D ho·∫∑c 3D ƒë·ªÉ d·ªÖ hi·ªÉu h∆°n. PCA gi√∫p b·∫°n l√†m ƒëi·ªÅu ƒë√≥!  

    ### üîπ **V√≠ d·ª• tr·ª±c quan**:
    H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu g·ªìm nhi·ªÅu ƒëi·ªÉm ph√¢n b·ªë theo m·ªôt ƒë∆∞·ªùng ch√©o trong kh√¥ng gian 2D:
    """)

   
    np.random.seed(42)
    x = np.random.rand(100) * 10  
    y = x * 0.8 + np.random.randn(100) * 2  
    X = np.column_stack((x, y))

    fig, ax = plt.subplots()
    ax.scatter(X[:, 0], X[:, 1], color="blue", alpha=0.5, label="D·ªØ li·ªáu ban ƒë·∫ßu")
    ax.set_xlabel("X1")
    ax.set_ylabel("X2")
    ax.legend()
    st.pyplot(fig)

    st.markdown("""
    D·ªØ li·ªáu n√†y c√≥ s·ª± ph√¢n t√°n r√µ r√†ng theo m·ªôt h∆∞·ªõng ch√≠nh. PCA s·∫Ω t√¨m ra h∆∞·ªõng ƒë√≥ ƒë·ªÉ bi·ªÉu di·ªÖn d·ªØ li·ªáu m·ªôt c√°ch t·ªëi ∆∞u.

    ### üîπ **C√°c b∆∞·ªõc th·ª±c hi·ªán PCA m·ªôt c√°ch d·ªÖ hi·ªÉu**:
    1Ô∏è‚É£ **T√¨m ƒëi·ªÉm trung t√¢m (mean vector)**  
       T√≠nh gi√° tr·ªã trung b√¨nh c·ªßa t·ª´ng c·ªôt (t·ª´ng chi·ªÅu d·ªØ li·ªáu).  
       
    2Ô∏è‚É£ **D·ªãch chuy·ªÉn d·ªØ li·ªáu v·ªÅ g·ªëc t·ªça ƒë·ªô**  
       Tr·ª´ m·ªói ƒëi·ªÉm d·ªØ li·ªáu ƒëi gi√° tr·ªã trung b√¨nh ƒë·ªÉ t·∫≠p trung d·ªØ li·ªáu quanh g·ªëc.  

    3Ô∏è‚É£ **T√≠nh ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai**  
       Hi·ªÉu ƒë∆°n gi·∫£n, ma tr·∫≠n n√†y ƒëo m·ª©c ƒë·ªô c√°c bi·∫øn thay ƒë·ªïi c√πng nhau.  

    4Ô∏è‚É£ **T√¨m c√°c h∆∞·ªõng quan tr·ªçng nh·∫•t**  
       - T√≠nh c√°c tr·ªã ri√™ng (eigenvalues) v√† vector ri√™ng (eigenvectors).  
       - Ch√∫ng cho ta bi·∫øt ƒë√¢u l√† h∆∞·ªõng quan tr·ªçng nh·∫•t c·ªßa d·ªØ li·ªáu.  

    5Ô∏è‚É£ **Chi·∫øu d·ªØ li·ªáu l√™n kh√¥ng gian m·ªõi**  
       - Ch·ªçn m·ªôt s·ªë h∆∞·ªõng ch√≠nh (principal components).  
       - Bi·ªÉu di·ªÖn d·ªØ li·ªáu theo c√°c tr·ª•c n√†y thay v√¨ tr·ª•c g·ªëc.  

    ### üîπ **Tr·ª±c quan h√≥a qu√° tr√¨nh PCA**
    D∆∞·ªõi ƒë√¢y l√† minh h·ªça c√°ch PCA t√¨m ra tr·ª•c quan tr·ªçng nh·∫•t c·ªßa d·ªØ li·ªáu:
    """)

    # PCA th·ªß c√¥ng
    X_centered = X - np.mean(X, axis=0)
    cov_matrix = np.cov(X_centered.T)
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

    fig, ax = plt.subplots()
    ax.scatter(X[:, 0], X[:, 1], color="blue", alpha=0.5, label="D·ªØ li·ªáu ban ƒë·∫ßu")
    origin = np.mean(X, axis=0)

    for i in range(2):
        ax.arrow(origin[0], origin[1], 
                 eigenvectors[0, i] * 3, eigenvectors[1, i] * 3, 
                 head_width=0.3, head_length=0.3, color="red", label=f"Tr·ª•c {i+1}")

    ax.set_xlabel("X1")
    ax.set_ylabel("X2")
    ax.legend()
    st.pyplot(fig)

    st.markdown("""
    **üîπ K·∫øt qu·∫£:**  
    - Tr·ª•c ƒë·ªè l√† h∆∞·ªõng m√† PCA t√¨m ra.  
    - N·∫øu ch·ªçn 1 tr·ª•c ch√≠nh, ta c√≥ th·ªÉ chi·∫øu d·ªØ li·ªáu l√™n n√≥ ƒë·ªÉ gi·∫£m chi·ªÅu.  
      
    Nh·ªù ƒë√≥, ch√∫ng ta c√≥ th·ªÉ bi·ªÉu di·ªÖn d·ªØ li·ªáu m·ªôt c√°ch g·ªçn g√†ng h∆°n m√† kh√¥ng m·∫•t qu√° nhi·ªÅu th√¥ng tin!  
    """)


def explain_tsne():
    st.markdown(r"""
    ## üåå t-Distributed Stochastic Neighbor Embedding (t-SNE)
    t-SNE l√† m·ªôt ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu m·∫°nh m·∫Ω, gi√∫p hi·ªÉn th·ªã d·ªØ li·ªáu ƒëa chi·ªÅu tr√™n m·∫∑t ph·∫≥ng 2D ho·∫∑c kh√¥ng gian 3D b·∫±ng c√°ch b·∫£o to√†n m·ªëi quan h·ªá gi·ªØa c√°c ƒëi·ªÉm g·∫ßn nhau.

    ### üîπ **Nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa t-SNE:**
    1. **T√≠nh x√°c su·∫•t ƒëi·ªÉm g·∫ßn nhau trong kh√¥ng gian g·ªëc:**  
       V·ªõi m·ªói ƒëi·ªÉm $x_i$, ta ƒë·ªãnh nghƒ©a x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán gi·ªØa $x_i$ v√† $x_j$ nh∆∞ sau:
       $$
       p_{j|i} = \frac{\exp(-\| x_i - x_j \|^2 / 2\sigma^2)}{\sum_{k \neq i} \exp(-\| x_i - x_k \|^2 / 2\sigma^2)}
       $$
       Trong ƒë√≥, $\sigma$ l√† tham s·ªë ·∫£nh h∆∞·ªüng ƒë·∫øn m·ª©c ƒë·ªô ph√¢n b·ªë c·ªßa ƒëi·ªÉm xung quanh.

    2. **T√≠nh x√°c su·∫•t trong kh√¥ng gian gi·∫£m chi·ªÅu (2D/3D):**  
       Trong kh√¥ng gian gi·∫£m chi·ªÅu, ta s·ª≠ d·ª•ng ph√¢n ph·ªëi t-Student v·ªõi m·ªôt m·ª©c ƒë·ªô t·ª± do:
       $$
       q_{j|i} = \frac{(1 + \| y_i - y_j \|^2)^{-1}}{\sum_{k \neq i} (1 + \| y_i - y_k \|^2)^{-1}}
       $$

    3. **T·ªëi ∆∞u h√≥a kho·∫£ng c√°ch gi·ªØa $p_{j|i}$ v√† $q_{j|i}$:**  
       t-SNE c·ªë g·∫Øng l√†m cho $p_{j|i}$ trong kh√¥ng gian g·ªëc g·∫ßn b·∫±ng $q_{j|i}$ trong kh√¥ng gian m·ªõi, b·∫±ng c√°ch gi·∫£m **h√†m m·∫•t m√°t Kullback-Leibler (KL divergence):**
       $$
       KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
       $$

    ### ‚úÖ **∆Øu ƒëi·ªÉm c·ªßa t-SNE**
    - Hi·ªÉn th·ªã c·ª•m d·ªØ li·ªáu t·ªët h∆°n PCA.
    - B·∫£o to√†n m·ªëi quan h·ªá phi tuy·∫øn t√≠nh.

    ### ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm c·ªßa t-SNE**
    - Ch·∫°y ch·∫≠m h∆°n PCA.
    - Nh·∫°y c·∫£m v·ªõi c√°c tham s·ªë nh∆∞ perplexity.
    """)
def thi_nghiem():
    
    st.title("üìâ Gi·∫£m chi·ªÅu d·ªØ li·ªáu MNIST v·ªõi PCA & t-SNE")

    # Load d·ªØ li·ªáu
    Xmt = np.load("buoi4/X.npy")
    ymt = np.load("buoi4/y.npy")
    X = Xmt.reshape(Xmt.shape[0], -1) 
    y = ymt.reshape(-1) 






    # T√πy ch·ªçn thu·∫≠t to√°n
    method = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu", ["PCA", "t-SNE"])
    n_components = st.slider("S·ªë chi·ªÅu gi·∫£m xu·ªëng", 2, 3, 2)

    # Gi·ªõi h·∫°n s·ªë m·∫´u ƒë·ªÉ tƒÉng t·ªëc (c√≥ th·ªÉ ch·ªânh s·ª≠a)
    num_samples = 5000
    X_subset, y_subset = X[:num_samples], y[:num_samples]

    # N√∫t ch·∫°y thu·∫≠t to√°n
    if st.button("üöÄ Ch·∫°y gi·∫£m chi·ªÅu"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            if method == "PCA":
                reducer = PCA(n_components=n_components)
            else:
                reducer = TSNE(n_components=n_components, perplexity=min(30, num_samples - 1), random_state=42)

            X_reduced = reducer.fit_transform(X_subset)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            if n_components == 2:
                fig = px.scatter(x=X_reduced[:, 0], y=X_reduced[:, 1], color=y_subset.astype(str),
                                title=f"{method} gi·∫£m chi·ªÅu xu·ªëng {n_components}D",
                                labels={'x': "Th√†nh ph·∫ßn 1", 'y': "Th√†nh ph·∫ßn 2"})
            else:  # Bi·ªÉu ƒë·ªì 3D
                fig = px.scatter_3d(x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2],
                                    color=y_subset.astype(str),
                                    title=f"{method} gi·∫£m chi·ªÅu xu·ªëng {n_components}D",
                                    labels={'x': "Th√†nh ph·∫ßn 1", 'y': "Th√†nh ph·∫ßn 2", 'z': "Th√†nh ph·∫ßn 3"})

            st.plotly_chart(fig)

    st.success("Ho√†n th√†nh!")
    
    
def pca_tsne():
        
    tab1, tab2, tab3 = st.tabs(["üìò L√Ω thuy·∫øt PCA", "üìò L√Ω thuy·∫øt t-NSE", "üìò Data"] )

    with tab1:
        explain_pca()

    with tab2:
        explain_tsne()
    
    with tab3:
        thi_nghiem()



if __name__ == "__main__":
    pca_tsne()