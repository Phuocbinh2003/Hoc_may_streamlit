import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.datasets import fetch_openml

def explain_pca():
    st.markdown(r"""
    ## üß† Principal Component Analysis (PCA)
    PCA l√† m·ªôt ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu d·ªØ li·ªáu b·∫±ng c√°ch t√¨m c√°c th√†nh ph·∫ßn ch√≠nh (principal components), t·ª©c l√† c√°c tr·ª•c m·ªõi sao cho d·ªØ li·ªáu ƒë∆∞·ª£c tr·∫£i r·ªông nh·∫•t theo c√°c h∆∞·ªõng n√†y.

    ### üîπ **C√°c b∆∞·ªõc th·ª±c hi·ªán PCA:**
    1. Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ c√≥ gi√° tr·ªã trung b√¨nh b·∫±ng 0.
    2. T√≠nh ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (Covariance Matrix):
       $$
       C = \frac{1}{n} X^T X
       $$
    3. T√≠nh gi√° tr·ªã ri√™ng ($\lambda$) v√† vector ri√™ng ($v$) c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai:
       $$
       C v = \lambda v
       $$
    4. Ch·ªçn $k$ vector ri√™ng t∆∞∆°ng ·ª©ng v·ªõi $k$ gi√° tr·ªã ri√™ng l·ªõn nh·∫•t ƒë·ªÉ t·∫°o kh√¥ng gian m·ªõi.
    5. Bi·ªÉu di·ªÖn d·ªØ li·ªáu trong kh√¥ng gian m·ªõi:
       $$
       X_{new} = X W
       $$
       v·ªõi $W$ l√† ma tr·∫≠n ch·ª©a c√°c vector ri√™ng.

    ### ‚úÖ **∆Øu ƒëi·ªÉm c·ªßa PCA**
    - Gi·∫£m chi·ªÅu nhanh ch√≥ng.
    - B·∫£o to√†n th√¥ng tin quan tr·ªçng trong d·ªØ li·ªáu.
    - Lo·∫°i b·ªè nhi·ªÖu trong d·ªØ li·ªáu.

    ### ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm c·ªßa PCA**
    - Kh√¥ng gi·ªØ ƒë∆∞·ª£c c·∫•u tr√∫c phi tuy·∫øn t√≠nh c·ªßa d·ªØ li·ªáu.
    - C√°c th√†nh ph·∫ßn ch√≠nh kh√¥ng d·ªÖ gi·∫£i th√≠ch v·ªÅ m·∫∑t √Ω nghƒ©a.
    """)


def explain_tsne():
    st.markdown(r"""
    ## üåå t-Distributed Stochastic Neighbor Embedding (t-SNE)
    t-SNE l√† m·ªôt ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu m·∫°nh m·∫Ω, gi√∫p hi·ªÉn th·ªã d·ªØ li·ªáu ƒëa chi·ªÅu tr√™n m·∫∑t ph·∫≥ng 2D ho·∫∑c kh√¥ng gian 3D b·∫±ng c√°ch b·∫£o to√†n m·ªëi quan h·ªá gi·ªØa c√°c ƒëi·ªÉm g·∫ßn nhau.

    ### üîπ **Nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa t-SNE:**
    1. **T√≠nh x√°c su·∫•t ƒëi·ªÉm g·∫ßn nhau trong kh√¥ng gian g·ªëc:**  
       V·ªõi m·ªói ƒëi·ªÉm $x_i$, ta ƒë·ªãnh nghƒ©a x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán gi·ªØa $x_i$ v√† $x_j$ nh∆∞ sau:
       $$
       p_{j|i} = \frac{\exp(-\| x_i - x_j \|^2 / 2\sigma^2)}{\sum_{k \neq i} \exp(-\| x_i - x_k \|^2 / 2\sigma^2)}
       $$
       Trong ƒë√≥, $\sigma$ l√† tham s·ªë ·∫£nh h∆∞·ªüng ƒë·∫øn m·ª©c ƒë·ªô ph√¢n b·ªë c·ªßa ƒëi·ªÉm xung quanh.

    2. **T√≠nh x√°c su·∫•t trong kh√¥ng gian gi·∫£m chi·ªÅu (2D/3D):**  
       Trong kh√¥ng gian gi·∫£m chi·ªÅu, ta s·ª≠ d·ª•ng ph√¢n ph·ªëi t-Student v·ªõi m·ªôt m·ª©c ƒë·ªô t·ª± do:
       $$
       q_{j|i} = \frac{(1 + \| y_i - y_j \|^2)^{-1}}{\sum_{k \neq i} (1 + \| y_i - y_k \|^2)^{-1}}
       $$

    3. **T·ªëi ∆∞u h√≥a kho·∫£ng c√°ch gi·ªØa $p_{j|i}$ v√† $q_{j|i}$:**  
       t-SNE c·ªë g·∫Øng l√†m cho $p_{j|i}$ trong kh√¥ng gian g·ªëc g·∫ßn b·∫±ng $q_{j|i}$ trong kh√¥ng gian m·ªõi, b·∫±ng c√°ch gi·∫£m **h√†m m·∫•t m√°t Kullback-Leibler (KL divergence):**
       $$
       KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
       $$

    ### ‚úÖ **∆Øu ƒëi·ªÉm c·ªßa t-SNE**
    - Hi·ªÉn th·ªã c·ª•m d·ªØ li·ªáu t·ªët h∆°n PCA.
    - B·∫£o to√†n m·ªëi quan h·ªá phi tuy·∫øn t√≠nh.

    ### ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm c·ªßa t-SNE**
    - Ch·∫°y ch·∫≠m h∆°n PCA.
    - Nh·∫°y c·∫£m v·ªõi c√°c tham s·ªë nh∆∞ perplexity.
    """)
def thi_nghiem():
    
    st.title("üìâ Gi·∫£m chi·ªÅu d·ªØ li·ªáu MNIST v·ªõi PCA & t-SNE")

    # Load d·ªØ li·ªáu
    Xmt = np.load("buoi4/X.npy")
    ymt = np.load("buoi4/y.npy")
    X = Xmt.reshape(Xmt.shape[0], -1) 
    y = ymt.reshape(-1) 






    # T√πy ch·ªçn thu·∫≠t to√°n
    method = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu", ["PCA", "t-SNE"])
    n_components = st.slider("S·ªë chi·ªÅu gi·∫£m xu·ªëng", 2, 3, 2)

    # Gi·ªõi h·∫°n s·ªë m·∫´u ƒë·ªÉ tƒÉng t·ªëc (c√≥ th·ªÉ ch·ªânh s·ª≠a)
    num_samples = 5000
    X_subset, y_subset = X[:num_samples], y[:num_samples]

    # N√∫t ch·∫°y thu·∫≠t to√°n
    if st.button("üöÄ Ch·∫°y gi·∫£m chi·ªÅu"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            if method == "PCA":
                reducer = PCA(n_components=n_components)
            else:
                reducer = TSNE(n_components=n_components, perplexity=min(30, num_samples - 1), random_state=42)

            X_reduced = reducer.fit_transform(X_subset)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            if n_components == 2:
                fig = px.scatter(x=X_reduced[:, 0], y=X_reduced[:, 1], color=y_subset.astype(str),
                                title=f"{method} gi·∫£m chi·ªÅu xu·ªëng {n_components}D",
                                labels={'x': "Th√†nh ph·∫ßn 1", 'y': "Th√†nh ph·∫ßn 2"})
            else:  # Bi·ªÉu ƒë·ªì 3D
                fig = px.scatter_3d(x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2],
                                    color=y_subset.astype(str),
                                    title=f"{method} gi·∫£m chi·ªÅu xu·ªëng {n_components}D",
                                    labels={'x': "Th√†nh ph·∫ßn 1", 'y': "Th√†nh ph·∫ßn 2", 'z': "Th√†nh ph·∫ßn 3"})

            st.plotly_chart(fig)

    st.success("Ho√†n th√†nh!")
    
    
def pca_tsne():
        
    tab1, tab2, tab3 = st.tabs(["üìò L√Ω thuy·∫øt PCA", "üìò L√Ω thuy·∫øt t-NSE", "üìò Data"] )

    with tab1:
        explain_pca()

    with tab2:
        explain_tsne()
    
    with tab3:
        thi_nghiem()



if __name__ == "__main__":
    pca_tsne()