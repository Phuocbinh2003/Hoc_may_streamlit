import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import mlflow
import os
import time
from datetime import datetime
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import plotly.express as px

# ======================================
# PH·∫¶N 1: L√ù THUY·∫æT NEURAL NETWORK
# ======================================
def explain_nn():
    st.markdown("""
    ## üß† Neural Network C∆° B·∫£n

    **Neural Network (M·∫°ng n∆°-ron)** l√† m√¥ h√¨nh l·∫•y c·∫£m h·ª©ng t·ª´ ho·∫°t ƒë·ªông c·ªßa n√£o b·ªô, g·ªìm nhi·ªÅu l·ªõp n∆°-ron nh√¢n t·∫°o k·∫øt n·ªëi v·ªõi nhau.

    ### üî∞ Ki·∫øn tr√∫c c∆° b·∫£n:
    """)
    st.image("buoi7/img1.webp", caption="M·ªôt s·ªë h√¨nh ·∫£nh t·ª´ MNIST Dataset", use_container_width=True)
    
    st.markdown("""
    ### üìå C√°c th√†nh ph·∫ßn ch√≠nh:
    1. **Input Layer**: L·ªõp ti·∫øp nh·∫≠n d·ªØ li·ªáu ƒë·∫ßu v√†o
    2. **Hidden Layers**: C√°c l·ªõp x·ª≠ l√Ω ·∫©n
    3. **Output Layer**: L·ªõp ƒë∆∞a ra k·∫øt qu·∫£ d·ª± ƒëo√°n

    ### üéØ H√†m k√≠ch ho·∫°t (Activation Functions):
    """)
    st.image("buoi7/img1.png", caption="M·ªôt s·ªë h√¨nh ·∫£nh t·ª´ MNIST Dataset", use_container_width=True)
    
    st.markdown("""
    ### üîÑ Qu√° tr√¨nh lan truy·ªÅn:
    1. **Lan truy·ªÅn thu·∫≠n (Forward Propagation)**: T√≠nh to√°n ƒë·∫ßu ra
    2. **T√≠nh to√°n loss**: So s√°nh v·ªõi gi√° tr·ªã th·ª±c
    3. **Lan truy·ªÅn ng∆∞·ª£c (Backpropagation)**: C·∫≠p nh·∫≠t tr·ªçng s·ªë
    4. **T·ªëi ∆∞u h√≥a**: S·ª≠ d·ª•ng c√°c thu·∫≠t to√°n nh∆∞ GD, Adam

    ### üìâ H√†m m·∫•t m√°t ph·ªï bi·∫øn:
    ```python
    Loss = CrossEntropy(y_true, y_pred)
    ```
    """)

    
    
    
    
    



# ======================================

def input_mlflow():
    DAGSHUB_MLFLOW_URI = "https://dagshub.com/Phuocbinh2003/Hoc_may_python.mlflow"
    mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
    st.session_state['mlflow_url'] = DAGSHUB_MLFLOW_URI
    os.environ["MLFLOW_TRACKING_USERNAME"] = "Phuocbinh2003"
    os.environ["MLFLOW_TRACKING_PASSWORD"] = "c1495823c8f9156923b06f15899e989db7e62052"
    mlflow.set_experiment("PCA_t-SNE")

import streamlit as st
import numpy as np
import tensorflow as tf
import mlflow
import mlflow.tensorflow
import time
from tensorflow import keras
from tensorflow.keras import layers

def thi_nghiem():
    st.title("üß† Hu·∫•n luy·ªán Neural Network tr√™n MNIST")

    # Load d·ªØ li·ªáu
    Xmt = np.load("buoi4/X.npy")
    ymt = np.load("buoi4/y.npy")
    X = Xmt.reshape(Xmt.shape[0], -1) / 255.0  # Chu·∫©n h√≥a d·ªØ li·ªáu v·ªÅ [0,1]
    y = ymt.reshape(-1)

    # L·ª±a ch·ªçn s·ªë l∆∞·ª£ng m·∫´u
    num_samples = st.slider("Ch·ªçn s·ªë l∆∞·ª£ng m·∫´u MNIST s·ª≠ d·ª•ng:", 1000, 60000, 5000, 1000)
    X_train, y_train = X[:num_samples], y[:num_samples]

    # C·∫•u h√¨nh m√¥ h√¨nh
    num_layers = st.slider("S·ªë l·ªõp ·∫©n:", 1, 5, 2)
    num_neurons = st.slider("S·ªë neuron m·ªói l·ªõp:", 32, 512, 128, 32)
    activation = st.selectbox("H√†m k√≠ch ho·∫°t:", ["relu", "sigmoid", "tanh"])
    optimizer = st.selectbox("Optimizer:", ["adam", "sgd", "rmsprop"])
    loss_fn = st.selectbox("H√†m m·∫•t m√°t:", ["sparse_categorical_crossentropy", "categorical_crossentropy"])
    batch_size = st.slider("Batch size:", 16, 128, 32, 16)
    epochs = st.slider("Epochs:", 5, 100, 20, 5)
    validation_split = st.slider("T·ªâ l·ªá validation:", 0.1, 0.5, 0.2, 0.05)

    run_name = st.text_input("üîπ Nh·∫≠p t√™n Run:", "Default_Run")
    st.session_state["run_name"] = run_name if run_name else "default_run"

    if st.button("üöÄ Hu·∫•n luy·ªán m√¥ h√¨nh"):
        with st.spinner("ƒêang hu·∫•n luy·ªán..."):
            mlflow.start_run(run_name=st.session_state["run_name"])
            mlflow.log_params({
                "num_layers": num_layers,
                "num_neurons": num_neurons,
                "activation": activation,
                "optimizer": optimizer,
                "loss_function": loss_fn,
                "batch_size": batch_size,
                "epochs": epochs,
                "validation_split": validation_split,
                "num_samples": num_samples
            })

            # X√¢y d·ª±ng m√¥ h√¨nh
            model = keras.Sequential([layers.Input(shape=(X_train.shape[1],))])
            for _ in range(num_layers):
                model.add(layers.Dense(num_neurons, activation=activation))
            model.add(layers.Dense(10, activation="softmax"))

            model.compile(optimizer=optimizer, loss=loss_fn, metrics=["accuracy"])

            start_time = time.time()
            history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, 
                                validation_split=validation_split, verbose=1)
            elapsed_time = time.time() - start_time
            mlflow.log_metric("elapsed_time", elapsed_time)

            # Log k·∫øt qu·∫£
            mlflow.log_metrics({
                "train_accuracy": history.history["accuracy"][-1],
                "val_accuracy": history.history["val_accuracy"][-1],
                "train_loss": history.history["loss"][-1],
                "val_loss": history.history["val_loss"][-1]
            })

            model.save("logs/mnist_model.h5")
            mlflow.log_artifact("logs/mnist_model.h5")

            mlflow.end_run()
            st.success(f"‚úÖ ƒê√£ log d·ªØ li·ªáu cho **Train_{st.session_state['run_name']}**!")

import streamlit as st
import numpy as np
import joblib
import random
import pandas as pd
import time
from PIL import Image
from streamlit_drawable_canvas import st_canvas
from tensorflow.keras.models import load_model

def preprocess_canvas_image(canvas_result):
    """Chuy·ªÉn ƒë·ªïi ·∫£nh t·ª´ canvas sang ƒë·ªãnh d·∫°ng ph√π h·ª£p ƒë·ªÉ d·ª± ƒëo√°n."""
    if canvas_result.image_data is None:
        return None
    img = canvas_result.image_data[:, :, :3]  # Ch·ªâ l·∫•y 3 k√™nh RGB
    img = Image.fromarray(img).convert("L").resize((28, 28))  # Chuy·ªÉn sang grayscale, resize v·ªÅ 28x28
    img = np.array(img) / 255.0  # Chu·∫©n h√≥a v·ªÅ [0,1]
    img = img.reshape(1, -1)  # ƒê∆∞a v·ªÅ d·∫°ng vector gi·ªëng nh∆∞ trong `thi_nghiem()`
    return img

def du_doan():
    st.header("‚úçÔ∏è V·∫Ω s·ªë ƒë·ªÉ d·ª± ƒëo√°n")

    # üîπ Danh s√°ch m√¥ h√¨nh c√≥ s·∫µn
    models = {
        "SVM Linear": "buoi4/svm_mnist_linear.joblib",
        "SVM Poly": "buoi4/svm_mnist_poly.joblib",
        "SVM Sigmoid": "buoi4/svm_mnist_sigmoid.joblib",
        "SVM RBF": "buoi4/svm_mnist_rbf.joblib",
    }

    # üìå Ch·ªçn m√¥ h√¨nh
    model_option = st.selectbox("üîç Ch·ªçn m√¥ h√¨nh:", list(models.keys()))

    # Load m√¥ h√¨nh t·ª´ file
    with st.spinner(f"üì• ƒêang t·∫£i m√¥ h√¨nh {model_option}..."):
        model = joblib.load(models[model_option])
    st.success(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh: {model_option}")

    # üÜï C·∫≠p nh·∫≠t key cho canvas khi nh·∫•n "T·∫£i l·∫°i"
    if "key_value" not in st.session_state:
        st.session_state.key_value = str(random.randint(0, 1000000))  

    if st.button("üîÑ T·∫£i l·∫°i n·∫øu kh√¥ng th·∫•y canvas"):
        st.session_state.key_value = str(random.randint(0, 1000000))  

    # ‚úçÔ∏è V·∫Ω s·ªë
    canvas_result = st_canvas(
        fill_color="black",
        stroke_width=10,
        stroke_color="white",
        background_color="black",
        height=150,
        width=150,
        drawing_mode="freedraw",
        key=st.session_state.key_value,
        update_streamlit=True
    )

    if st.button("D·ª± ƒëo√°n s·ªë"):
        img = preprocess_canvas_image(canvas_result)

        if img is not None:
            st.image(Image.fromarray((img.reshape(28, 28) * 255).astype(np.uint8)), caption="·∫¢nh sau x·ª≠ l√Ω", width=100)

            # D·ª± ƒëo√°n s·ªë
            prediction = model.predict(img)
            confidence_scores = model.decision_function(img)  

            # Chuy·ªÉn ƒë·ªïi th√†nh x√°c su·∫•t softmax
            confidence_scores = np.exp(confidence_scores) / np.sum(np.exp(confidence_scores), axis=1, keepdims=True)

            predicted_number = prediction[0]
            max_confidence = np.max(confidence_scores)

            st.subheader(f"üî¢ D·ª± ƒëo√°n: {predicted_number}")
            st.write(f"üìä M·ª©c ƒë·ªô tin c·∫≠y: {max_confidence:.2%}")

            # Hi·ªÉn th·ªã b·∫£ng confidence scores
            prob_df = pd.DataFrame(confidence_scores.reshape(1, -1), columns=[str(i) for i in range(10)]).T
            prob_df.columns = ["M·ª©c ƒë·ªô tin c·∫≠y"]
            st.bar_chart(prob_df)

        else:
            st.error("‚ö†Ô∏è H√£y v·∫Ω m·ªôt s·ªë tr∆∞·ªõc khi b·∫•m D·ª± ƒëo√°n!")
    
from datetime import datetime    
import streamlit as st
import mlflow
from datetime import datetime

def show_experiment_selector():
    st.title("üìä MLflow")
    
    # K·∫øt n·ªëi v·ªõi DAGsHub MLflow Tracking
    mlflow.set_tracking_uri("https://dagshub.com/Phuocbinh2003/Hoc_may_python.mlflow")
    
    # L·∫•y danh s√°ch t·∫•t c·∫£ experiments
    experiment_name = "PCA_t-SNE"
    experiments = mlflow.search_experiments()
    selected_experiment = next((exp for exp in experiments if exp.name == experiment_name), None)

    if not selected_experiment:
        st.error(f"‚ùå Experiment '{experiment_name}' kh√¥ng t·ªìn t·∫°i!")
        return

    st.subheader(f"üìå Experiment: {experiment_name}")
    st.write(f"**Experiment ID:** {selected_experiment.experiment_id}")
    st.write(f"**Tr·∫°ng th√°i:** {'Active' if selected_experiment.lifecycle_stage == 'active' else 'Deleted'}")
    st.write(f"**V·ªã tr√≠ l∆∞u tr·ªØ:** {selected_experiment.artifact_location}")

    # L·∫•y danh s√°ch runs trong experiment
    runs = mlflow.search_runs(experiment_ids=[selected_experiment.experiment_id])

    if runs.empty:
        st.warning("‚ö† Kh√¥ng c√≥ runs n√†o trong experiment n√†y.")
        return

    st.write("### üèÉ‚Äç‚ôÇÔ∏è C√°c Runs g·∫ßn ƒë√¢y:")
    
    # L·∫•y danh s√°ch run_name t·ª´ params
    run_info = []
    for _, run in runs.iterrows():
        run_id = run["run_id"]
        run_params = mlflow.get_run(run_id).data.params
        run_name = run_params.get("run_name", f"Run {run_id[:8]}")
        run_info.append((run_name, run_id))
    
    # T·∫°o dictionary ƒë·ªÉ map run_name -> run_id
    run_name_to_id = dict(run_info)
    run_names = list(run_name_to_id.keys())
    
    # Ch·ªçn run theo run_name
    selected_run_name = st.selectbox("üîç Ch·ªçn m·ªôt run:", run_names)
    selected_run_id = run_name_to_id[selected_run_name]

    # Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt c·ªßa run ƒë∆∞·ª£c ch·ªçn
    selected_run = mlflow.get_run(selected_run_id)

    if selected_run:
        st.subheader(f"üìå Th√¥ng tin Run: {selected_run_name}")
        st.write(f"**Run ID:** {selected_run_id}")
        st.write(f"**Tr·∫°ng th√°i:** {selected_run.info.status}")
        
        start_time_ms = selected_run.info.start_time  # Th·ªùi gian l∆∞u d∆∞·ªõi d·∫°ng milliseconds
        if start_time_ms:
            start_time = datetime.fromtimestamp(start_time_ms / 1000).strftime("%Y-%m-%d %H:%M:%S")
        else:
            start_time = "Kh√¥ng c√≥ th√¥ng tin"
        
        st.write(f"**Th·ªùi gian ch·∫°y:** {start_time}")

        # Hi·ªÉn th·ªã th√¥ng s·ªë ƒë√£ log
        params = selected_run.data.params
        metrics = selected_run.data.metrics

        if params:
            st.write("### ‚öôÔ∏è Parameters:")
            st.json(params)

        if metrics:
            st.write("### üìä Metrics:")
            st.json(metrics)

        # Ki·ªÉm tra v√† hi·ªÉn th·ªã dataset artifact
        dataset_path = f"{selected_experiment.artifact_location}/{selected_run_id}/artifacts/dataset.npy"
        st.write("### üìÇ Dataset:")
        st.write(f"üì• [T·∫£i dataset]({dataset_path})")
    else:
        st.warning("‚ö† Kh√¥ng t√¨m th·∫•y th√¥ng tin cho run n√†y.")

        
        
        
          
import mlflow
import os
from mlflow.tracking import MlflowClient
def pca_tsne():
    #st.title("üöÄ MLflow DAGsHub Tracking v·ªõi Streamlit")
    
    
    
    tab1, tab2, tab3,tab4 = st.tabs(["üìò L√Ω thuy·∫øt TRAINING NEURAL NETWORK", "üìò TRAINING NEURAL NETWORK", "DEMO","üî• Mlflow"] )

    with tab1:
        explain_nn()

    with tab2:
        thi_nghiem()
    
    with tab3:
        du_doan()
    with tab4:
        show_experiment_selector()


if __name__ == "__main__":
    pca_tsne()